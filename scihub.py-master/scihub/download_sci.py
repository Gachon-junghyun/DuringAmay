import os
import requests
from bs4 import BeautifulSoup
from urllib.parse import quote
import logging

# í•„ìš”í•˜ë©´ pip install retrying
from retrying import retry

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class CaptchaNeedException(Exception):
    pass

class SciHub:
    def __init__(self):
        self.available_base_url_list = [
            "https://sci-hub.se",
            "https://sci-hub.st",
            "https://sci-hub.ru",
        ]
        self.base_url = self.available_base_url_list[0]
        self.sess = requests.Session()

    def _change_base_url(self):
        self.available_base_url_list.append(self.available_base_url_list.pop(0))
        self.base_url = self.available_base_url_list[0]
        logger.warning(f"ğŸ” Sci-Hub URL ë³€ê²½ë¨ â†’ {self.base_url}")

    def _classify(self, identifier):
        if identifier.startswith("http") and identifier.endswith(".pdf"):
            return 'url-direct'
        elif identifier.startswith("10."):
            return 'doi'
        else:
            return 'url'

    def _search_direct_url(self, identifier):
        url = f"{self.base_url}/{quote(identifier)}"
        logger.info(f"ğŸ” Sci-Hub ê²€ìƒ‰ ì¤‘: {url}")
        res = self.sess.get(url, verify=False, timeout=10)
        soup = BeautifulSoup(res.content, "html.parser")
        iframe = soup.find("iframe")

        if iframe:
            src = iframe.get("src")
            if src.startswith("//"):
                src = "https:" + src
            elif src.startswith("/"):
                src = self.base_url + src
            return src
        else:
            raise CaptchaNeedException("CAPTCHA ë˜ëŠ” PDF iframe ì—†ìŒ")

    def _generate_name(self, res):
        cd = res.headers.get('Content-Disposition', '')
        if 'filename=' in cd:
            return cd.split('filename=')[-1].strip('"; ')
        return "downloaded_paper.pdf"

    def _save(self, content, filepath):
        with open(filepath, 'wb') as f:
            f.write(content)
        logger.info(f"âœ… ì €ì¥ ì™„ë£Œ: {filepath}")

    @retry(wait_random_min=1000, wait_random_max=3000, stop_max_attempt_number=5)
    def download(self, identifier, destination='pdfs', path=None):
        data = self.fetch(identifier)

        if 'err' not in data:
            filename = path if path else data['name']
            os.makedirs(destination, exist_ok=True)
            filepath = os.path.join(destination, filename)
            self._save(data['pdf'], filepath)

        return data

    def fetch(self, identifier):
        try:
            url = self._get_direct_url(identifier)
            res = self.sess.get(url, verify=False, timeout=10)

            if res.headers['Content-Type'] != 'application/pdf':
                logger.warning("âŒ CAPTCHA ë˜ëŠ” ì˜ëª»ëœ ì‘ë‹µ")
                self._change_base_url()
                raise CaptchaNeedException("PDF ì‘ë‹µì´ ì•„ë‹˜")

            return {
                'pdf': res.content,
                'url': url,
                'name': self._generate_name(res)
            }

        except Exception as e:
            logger.warning(f"â— ìš”ì²­ ì‹¤íŒ¨: {e}")
            return {
                'err': str(e)
            }

    def _get_direct_url(self, identifier):
        id_type = self._classify(identifier)
        return identifier if id_type == 'url-direct' else self._search_direct_url(identifier)
    
import requests

def get_doi_from_semantic_url(semantic_id):
    url = f"https://api.semanticscholar.org/graph/v1/paper/{semantic_id}"
    params = {
        "fields": "title,doi"
    }

    res = requests.get(url)
    if res.status_code != 200:
        print("âŒ DOI ì¡°íšŒ ì‹¤íŒ¨:", res.status_code)
        return None

    data = res.json()
    doi = data.get("doi")
    print(f"ğŸ“„ ì œëª©: {data.get('title')}")
    print(f"ğŸ”— DOI: {doi}")
    return doi


if __name__ == "__main__":
    
    semantic_id = "e27073dfe39bed1e9e0e059cddccb351f3df059a"
    doi = get_doi_from_semantic_url(semantic_id)

    print(doi)

    scihub = SciHub()
    # DOI ë˜ëŠ” ë…¼ë¬¸ URL ì…ë ¥
    identifier = "10.1109/5.771073"  # ë˜ëŠ” Semantic Scholar ë“±ì—ì„œ ê°€ì ¸ì˜¨ DOI

    result = scihub.download(identifier, destination="pdfs")

    if 'err' in result:
        print(f"âŒ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {result['err']}")
    else:
        print(f"âœ… ë‹¤ìš´ë¡œë“œ ì„±ê³µ: {result['name']}")

