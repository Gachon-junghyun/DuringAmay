from flask import Flask, request, jsonify
from flask_cors import CORS

from langchain.memory import ConversationBufferMemory
from langchain_openai import ChatOpenAI
from langchain.prompts import PromptTemplate
from langchain.chains.openai_functions import create_structured_output_chain
from pydantic import BaseModel
import os
from dotenv import load_dotenv

# âœ… í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()
openai_api_key = os.getenv("OPENAI_API_KEY")

app = Flask(__name__)
CORS(app)

# ğŸ’¡ ì£¼ì œ ì„¤ì •
TOPIC = "ì¸ê³µì§€ëŠ¥ì€ ì¸ê°„ì˜ ì°½ì˜ì„±ì„ ë„˜ì–´ì„¤ ìˆ˜ ìˆëŠ”ê°€?"

# âœ… Pydantic BaseModel ì •ì˜
class DebateResponse(BaseModel):
    respond: str
    mind: str

# ğŸ™ï¸ í† ë¡ ì í”„ë¡¬í”„íŠ¸ (JSON ì¶œë ¥ ìœ ë„)
debater_prompt = PromptTemplate(
    input_variables=["topic", "chat_history", "mind"],
    template="""
ë‹¹ì‹ ì€ ì—´ì •ì ì¸ í† ë¡ ìì…ë‹ˆë‹¤.

ì£¼ì œ: "{topic}"
ì ‘ê·¼ ê´€ì  (mind): "{mind}"

ì§€ê¸ˆê¹Œì§€ì˜ ëŒ€í™”:
{chat_history}

ìœ„ì˜ ê´€ì (mind)ì„ ë°”íƒ•ìœ¼ë¡œ, ì´ ì£¼ì œì— ëŒ€í•´ ë…¼ë¦¬ì ìœ¼ë¡œ ë°˜ì‘(respond)í•˜ì‹­ì‹œì˜¤.
ì•„ë˜ í˜•ì‹ìœ¼ë¡œ JSONìœ¼ë¡œ ì¶œë ¥í•˜ì‹­ì‹œì˜¤:

{{
  "respond": "...ë‹¹ì‹ ì˜ ì£¼ì¥...",
  "mind": "...ì´ ì£¼ì¥ì— ë‹´ê¸´ ì „ëµ/ì‚¬ê³  ë°©ì‹ ìš”ì•½..."
}}
"""
)

# âš–ï¸ ì¤‘ì¬ì í”„ë¡¬í”„íŠ¸
moderator_prompt = PromptTemplate(
    input_variables=["topic", "chat_history"],
    template="""
ë‹¹ì‹ ì€ ì¤‘ë¦½ì ì¸ ì¤‘ì¬ìì…ë‹ˆë‹¤.
ì£¼ì œ: "{topic}"

{chat_history}

ê°„ëµí•˜ê²Œ ìš”ì•½í•˜ê³  ë‹¤ìŒ ë°œì–¸ìë¥¼ ì§€ëª©í•´ì£¼ì„¸ìš”.
"""
)

# ğŸ¤– í† ë¡ ì ì„¤ì •
NUM_DEBATERS = 2
temperatures = [0.7] * NUM_DEBATERS
structured_debater_chains = []
debater_memories = []

for i in range(NUM_DEBATERS):
    llm = ChatOpenAI(model_name="gpt-4", temperature=temperatures[i], openai_api_key=openai_api_key)
    memory = ConversationBufferMemory(return_messages=True, memory_key="chat_history")
    chain = create_structured_output_chain(
        output_schema=DebateResponse,
        llm=llm,
        prompt=debater_prompt,
    )
    structured_debater_chains.append(chain)
    debater_memories.append(memory)

# ğŸ§‘â€âš–ï¸ ì¤‘ì¬ì ì„¤ì •
moderator_llm = ChatOpenAI(model_name="gpt-4", temperature=0.3, openai_api_key=openai_api_key)
moderator_memory = ConversationBufferMemory(return_messages=True, memory_key="chat_history")
moderator_chain = create_structured_output_chain(
    output_schema=DebateResponse,  # ì¤‘ì¬ìë„ ë™ì¼í•œ êµ¬ì¡° ì‚¬ìš©í•  ìˆ˜ ìˆìŒ
    llm=moderator_llm,
    prompt=moderator_prompt
)

# ğŸ§  ìƒíƒœ ê´€ë¦¬ìš© ì „ì—­ ë¼ìš´ë“œ
current_round = 1
max_rounds = 5

@app.route('/api/debate/next', methods=['POST'])
def debate_next():
    data = request.get_json()
    topic = data.get('topic', TOPIC)
    messages = data.get('messages', [])
    current_speaker = data.get('currentSpeaker')
    model = data.get('model')
    mind = data.get('mind', '')

    chat_history = "\n".join([f"[{m['senderName']}] {m['content']}" for m in messages])

    if current_speaker == "moderator":
        response_obj = moderator_chain.run(topic=topic, chat_history=chat_history)
        return jsonify({"response": response_obj.respond, "mind": response_obj.mind})
    else:
        debater_id = int(current_speaker) - 1
        chain = structured_debater_chains[debater_id]
        response_obj = chain.run(topic=topic, chat_history=chat_history, mind=mind)
        return jsonify({"response": response_obj.respond, "mind": response_obj.mind})


@app.route('/api/debate/advance_round', methods=['POST'])
def advance_round():
    global current_round
    if current_round < max_rounds:
        current_round += 1
    return jsonify({"round": current_round})


@app.route('/api/debate/reset', methods=['POST'])
def reset_debate():
    global current_round
    current_round = 1
    moderator_memory.clear()
    for mem in debater_memories:
        mem.clear()
    return jsonify({"status": "reset complete"})


if __name__ == '__main__':
    app.run(debug=True)
